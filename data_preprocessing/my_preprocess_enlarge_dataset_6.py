import numpy as np
import os
import re
import random
import pandas as pd
#from stanfordcorenlp import StanfordCoreNLP

# æ­£åˆ™è¡¨è¾¾å¼
p1 = r'^[\.!@#\$%\\\^&\*\)\(\+=\{\}\[\]\/\",\'\â€œ\â€\â€™<>~\Â·`\?:;|\-\s]{2,}$'
p2 = r'[^a-zA-Z0-9\.!@#\$%\\\^&\*\)\(\+=\{\}\[\]\/\",\'\â€œ\â€\â€™<>~\Â·`\?:;|\-\s]+'
p3 = r'(\w+?)/(\w+)'  # \w=[A-Za-z0-9_]
p4 = r'(.+)@(.+)'
p5 = r'([A-Z]*[a-z]+)\.([A-Z]*[a-z]+)'  # éå¤§å†™å­—æ¯
p6 = r'(\w+)\?(\w+)'
p7 = r'[A-Z]+'
p8 = r'[ğŸ˜„ğŸš«ğŸš¨ğŸ™‹ğŸ™„ğŸ™ƒğŸ˜­ğŸ˜©ğŸ˜¨ğŸ˜¦ğŸ˜¥ğŸ˜¤ğŸ˜ ğŸ˜•ğŸ˜”ğŸ˜„ğŸ‘ºğŸ‘ŒğŸ¸ğŸ¸ğŸâŒâ™¡â˜•ï¸â˜•â–ºâ€¦Ğ½ĞµğŸ˜±ğŸ˜ªğŸ‘ğŸ‘ğŸ‘‡â¤ï¸ğŸ˜´ğŸ˜‚ğŸ™ğŸ’”ğŸ’œğŸ˜’]+'
p9 = r'(.*)urlurlurl(.+)'
p10 = r'(.+)urlurlurl(.*)'
p11 = r'[\.!@#\$%\\\^&\*\)\(\+=\{\}\[\]\/\",\'\â€œ\â€\â€™<>~\Â·`\?:;|\-\s]+(\d+)'
p12 = r'[\s\n\r\t]+'
p13 = r'@(.+)'


def eng_word_tokenize(nlp):
    new_dataset = np.load('new_support_data.npy', encoding='bytes')
    new_tokenized_tweets = []
    for tw in new_dataset:
        #tw['p'] = nlp.word_tokenize(tw['p'])
        #tw['h'] = nlp.word_tokenize(tw['h'])
        tw['text'] = nlp.word_tokenize(tw['text'])
        new_tokenized_tweets.append(tw)
    np.save('new_support_tokenized_data', new_tokenized_tweets)


def clean_tokenized_text():
    new_tokenized_data = np.load('documents/new_support_tokenized_data.npy')
    cleaned_tweets = []
    for i, tw in enumerate(new_tokenized_data):
        #tw['p']=post_process(tw['p'])
        #tw['h']=post_process(tw['h'])
        tw['text'] = post_process(tw['text'])
        cleaned_tweets.append(tw)
    return cleaned_tweets


def post_process(text):
    for k, word in enumerate(text):
        # stanford corenlp result post-processing
        # å»ç©º
        if not text[k] or re.match(p12, text[k]):  # è¿™å›åŒ¹é…åˆ°4ä¸ªï¼Œçœ‹è¯è¡¨é‡Œè¿˜ä¼šä¸ä¼šå‡ºç°ç©º
            text.pop(k)
            if k >= len(text):
                break
        # å»é™¤è¡¨æƒ…ç¬¦å·å’Œä¸å¸¸è§æ ‡ç‚¹
        # åªæœ‰popæ²¡æœ‰insertå°±è‡ªåŠ¨è¿›å…¥ä¸‹ä¸€ä¸ªè¯äº†ï¼Œå…¶ä»–ç­›é€‰æ¡ä»¶éœ€è¦åœ¨è¿™ä¹‹å
        if re.match(p2, text[k]):
            text.pop(k)
            if k >= len(text):
                break
        if re.match(p8, text[k]) or text[k] == '':  # ä¸€äº›è¡¨æƒ…ç¬¦å·æ²¡å»é™¤å¹²å‡€çš„ï¼Œäººå·¥å»é™¤ï¼ˆåˆç­›é™¤101ä¸ªï¼‰
            text.pop(k)
            if k >= len(text):
                break

        # æŠŠç½‘å€è½¬æ¢æˆ 'urlurlurl'
        if 'http' in text[k] or 'www.' in text[k] or '.com' in text[
            k]:  # åªæœ‰ç¬¬ä¸€ä¸ªå¯ä»¥ç”¨wordï¼Œåé¢åªèƒ½ä¿®æ”¹äº†çš„å½“å‰ä½ç½®çš„è¯
            text[k] = 'urlurlurl'

        # æŒ‰/åˆ‡å¼€
        a = re.match(p3, text[k])
        temp_i = k
        while a:
            flag = 1
            text.pop(temp_i)
            text.insert(temp_i, a.group(1))
            temp_i += 1
            text.insert(temp_i, '/')
            temp_i += 1
            latter = a.group(2)
            a = re.match(p3, latter)
        if temp_i > k:
            text.insert(temp_i, latter)

        # æŒ‰@åˆ‡å¼€
        b = re.match(p4, text[k])
        if b:
            text.pop(k)
            text.insert(k, b.group(1))
            text.insert(k + 1, '@' + b.group(2))
        # æŒ‰.åˆ‡å¼€
        c = re.match(p5, text[k])
        temp_i = k
        while c:
            text.pop(temp_i)
            text.insert(temp_i, c.group(1))
            temp_i += 1
            text.insert(temp_i, '.')
            temp_i += 1
            latter = c.group(2)
            c = re.match(p5, latter)
        if temp_i > k:
            text.insert(temp_i, latter)
        # æŒ‰?åˆ‡å¼€
        d = re.match(p6, text[k])
        if d:
            text.pop(k)
            text.insert(k, d.group(1))
            text.insert(k + 1, '?')
            text.insert(k + 2, d.group(2))
        # æŠŠ'urlurlurl'å’Œå…¶ä»–çš„å†…å®¹åˆ‡åˆ†å¼€
        e = re.match(p9, text[k])
        if e:
            text.pop(k)
            if e.group(1):
                text.insert(k, e.group(1))
                text.insert(k + 1, 'urlurlurl')
                text.insert(k + 2, e.group(2))
            else:
                text.insert(k, 'urlurlurl')
                text.insert(k + 1, e.group(2))
        f = re.match(p10, text[k])
        if f:
            text.pop(k)
            text.insert(k, f.group(1))
            text.insert(k + 1, 'urlurlurl')
            if f.group(2):
                text.insert(k + 2, f.group(2))
        # æŠŠå¤šä¸ªè¿èµ·æ¥çš„æ ‡ç‚¹ç¬¦å·åˆ‡åˆ†å¼€
        if re.match(p1, text[k]):
            temp_word = text[k]
            text.pop(k)
            l = len(temp_word)
            for x in range(l):
                text.insert(k + x, temp_word[x])
        # å°†æ‰€æœ‰å¸¦å¤§å†™å­—æ¯çš„è¯è½¬ä¸ºå°å†™
        if re.findall(p7, text[k]):
            text[k] = text[k].lower()
        # æŠŠ@userè½¬åŒ–æˆ@
        if re.match(p1, text[k]):
            text.pop(k)
            text.insert(k, '@')
    return text


def examine_new_tweets(cleaned_tweets):
    # labels: 0:support,1:comment,2:deny,3:query
    premise = []
    hypothesis = []
    labels = []
    ids = []
    random.shuffle(cleaned_tweets)
    for i,tw in enumerate(cleaned_tweets):
        premise.append(tw['text'])
        hypothesis.append(tw['text'])
        labels.append(0)  # æ›¿æ¢æˆå„ç±»labelå¯¹åº”çš„æ•°å­—
        ids.append(i)
    # æ”¾åˆ°devé‡Œåˆ¤æ–­æ˜¯å¦è¾“å‡ºå’Œé¢„æœŸç›¸åŒçš„label
    np.save('data/taskA_examine_support_3/dev/A_premise', premise)
    np.save('data/taskA_examine_support_3/dev/A_hypothesis', hypothesis)
    np.save('data/taskA_examine_support_3/dev/A_labels', labels)
    np.save('data/taskA_examine_support_3/dev/A_id_set', ids)



def combine_new_tweets():
    # labels: 0:support,1:comment,2:deny,3:query
    # filter new data
    all_new_premise = np.load('data/taskA_examine_support_3/dev/A_premise.npy')
    all_new_hypothesis = np.load('data/taskA_examine_support_3/dev/A_hypothesis.npy')
    all_new_labels = np.load('data/taskA_examine_support_3/dev/A_labels.npy')
    filtered_new_tweets = []
    f = open('documents/support_examine_3.tsv', 'r')
    lines = f.readlines()[1:]
    f.close()
    for i in range(len(lines)):
        line = lines[i].strip()
        result = re.match(r'([a-z\d]+)\s(\d)\s\[(.*?)\]', line)
        if int(result.group(2))==0:  # æ›¿æ¢æˆå„ç±»labelå¯¹åº”çš„æ•°å­—
            tw = {}
            id_str = int(result.group(1))
            tw['p'] = all_new_premise[id_str]
            tw['h'] = all_new_hypothesis[id_str]
            tw['label'] = all_new_labels[id_str]
            filtered_new_tweets.append(tw)

    # merge new & old train data  --åœ¨åŠ äº†ä¸€ä¸ªç±»çš„åŸºç¡€ä¸ŠåŠ å¦ä¸€ä¸ªç±»çš„æ•°æ®
    premise = np.load('data/taskA_add_deny_1/train/A_premise.npy').tolist()
    hypothesis = np.load('data/taskA_add_deny_1/train/A_hypothesis.npy').tolist()
    labels = np.load('data/taskA_add_deny_1/train/A_labels.npy').tolist()
    for tw in filtered_new_tweets:
        premise.append(tw['p'])
        hypothesis.append(tw['h'])
        labels.append(tw['label'])

    # re-organize merged data
    reorganized_tweets = []
    for i in range(len(premise)):
        tweet={}
        tweet['p']=premise[i]
        tweet['h']=hypothesis[i]
        tweet['label']=labels[i]
        reorganized_tweets.append(tweet)
    random.shuffle(reorganized_tweets)

    # save reorganized merged data
    all_premise = []
    all_hypothesis = []
    all_labels = []
    for i in range(len(reorganized_tweets)):
        all_premise.append(reorganized_tweets[i]['p'])
        all_hypothesis.append(reorganized_tweets[i]['h'])
        all_labels.append(reorganized_tweets[i]['label'])
    np.save('data/taskA_add_support_3/train/A_premise', all_premise)
    np.save('data/taskA_add_support_3/train/A_hypothesis', all_hypothesis)
    np.save('data/taskA_add_support_3/train/A_labels', all_labels)


def error_analysis():
    #dev_hs = np.load('data/taskA_add_support_3/dev/A_hypothesis.npy')
    #dev_labels = np.load('data/taskA_add_support_3/dev/A_labels.npy')
    dev_hs = np.load('data/taskB_data_0102_full/dev/B_hypothesis.npy')
    dev_labels = np.load('data/taskB_data_0102_full/dev/B_labels.npy')
    dev_branches = np.load('data/taskA_data_0103/dev/tokenized_branch_id_A_B_label_stanford.npy')
    f = open('error_analysis/taskB_medium_7630.tsv', 'r')
    lines = f.readlines()[1:]
    f.close()
    ids = []
    texts = []
    #source_texts = []
    predict_labels = []
    true_labels = []
    A_labels = {0:'support',1:'comment',2:'deny',3:'query'}
    B_labels = {0: 'true', 1: 'false', 2: 'unverified'}
    for i in range(len(lines)):
        line = lines[i].strip()
        result = re.match(r'([a-z\d]+)\s(\d)\s\[(.*?)\]', line)
        pred_label = int(result.group(2))
        true_label = int(dev_labels[i])
        if pred_label != true_label:
            id_str = result.group(1)
            ids.append(id_str)
            my_str = ' '
            '''
            # taskA
            src=''
            for branch in dev_branches:
                for tw in branch:
                    if tw['id']==id_str:
                        src=branch[0]['text']
                        source_texts.append(my_str.join(src))
                        break
                if src:
                    break
            '''
            texts.append(my_str.join(dev_hs[i]))
            predict_labels.append(B_labels[pred_label])
            true_labels.append(B_labels[true_label])

    #print(len(source_texts))
    #print(len(texts))

    # å†™å…¥csv
    # å­—å…¸ä¸­çš„keyå€¼å³ä¸ºcsvä¸­åˆ—å
    '''
    # taskA
    dataframe = pd.DataFrame(
        {'id': ids, 'text': texts, 'source_texts':source_texts,'true_label': true_labels,
         'pred_label': predict_labels})
    '''
    dataframe = pd.DataFrame(
        {'id': ids, 'text': texts, 'true_label': true_labels,
         'pred_label': predict_labels})

    # å°†DataFrameå­˜å‚¨ä¸ºcsv,indexè¡¨ç¤ºæ˜¯å¦æ˜¾ç¤ºè¡Œåï¼Œdefault=True
    dataframe.to_csv("error_analysis/error_taskB_medium_7630.csv",index=True,sep=',')


if __name__ == "__main__":
    # åœ¨31ä¸Šåˆ†è¯
    #nlp = StanfordCoreNLP(r'/home/ruoyao/ruoyao/stanford-corenlp-full-2018-10-05')
    #eng_word_tokenize(nlp)
    # å¤„ç†åè®©GPTé¢„æµ‹ç­›é€‰
    #cleaned_tweets = clean_tokenized_text()
    #examine_new_tweets(cleaned_tweets)
    # ç­›é€‰ååŠ å…¥åˆ°åŸæœ¬çš„è®­ç»ƒæ•°æ®é‡Œ
    #combine_new_tweets()
    # é”™è¯¯åˆ†æ
    #error_analysis()